{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c410d9e-fb88-4920-a821-10e0aa6da230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a54097-cf36-4060-9c45-486e017b1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(\"project-20-files\")\n",
    "dataframes = {}\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(\"project-20-files\", file_name) \n",
    "    df_name = os.path.splitext(file_name)[0]  \n",
    "    dataframes[df_name] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "907a7cc3-05dc-4d1e-bf44-2b59ac844143",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataframes = {key: df for key, df in dataframes.items() if \"learn\" in key.lower()}\n",
    "\n",
    "merged_training = None\n",
    "\n",
    "for name, df in learn_dataframes.items():\n",
    "    if merged_training is None:\n",
    "        # Use the first DataFrame as the base\n",
    "        merged_training = df\n",
    "    else:\n",
    "        # Merge with the next DataFrame\n",
    "        merged_training = pd.merge(merged_training, df, on=\"pkey\", how=\"outer\")\n",
    "\n",
    "test_dataframes = {key: df for key, df in dataframes.items() if \"test\" in key.lower()}\n",
    "\n",
    "merged_test = None\n",
    "\n",
    "for name, df in test_dataframes.items():\n",
    "    if merged_test is None:\n",
    "        # Use the first DataFrame as the base\n",
    "        merged_test = df\n",
    "    else:\n",
    "        # Merge with the next DataFrame\n",
    "        merged_test = pd.merge(merged_test, df, on=\"pkey\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65a490b-6fa5-40e9-b60c-bc3090fb9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframes that contain \"city\" in their key\n",
    "city_dataframes = {key: df for key, df in dataframes.items() if \"city\" in key.lower()}\n",
    "\n",
    "# Initialize the merged DataFrame\n",
    "merged_city = None\n",
    "\n",
    "# Loop through the filtered DataFrames and merge them on 'INSEE'\n",
    "for name, df in city_dataframes.items():\n",
    "    if merged_city is None:\n",
    "        # Use the first DataFrame as the base\n",
    "        merged_city = df\n",
    "    else:\n",
    "        # Merge with the next DataFrame on the 'INSEE' column\n",
    "        merged_city = pd.merge(merged_city, df, on=\"INSEE\", how=\"outer\")\n",
    "\n",
    "# Resulting merged DataFrame: merged_city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7acaa2-e5c1-4686-8802-c3a598ce5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = pd.read_csv(\"project-20-files/departments.csv\")\n",
    "reg = pd.read_csv(\"project-20-files/regions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82730ebc-c02c-4c93-814c-31c23a5ba17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    data = pd.merge(data,merged_city,on = \"INSEE\", how = \"inner\")\n",
    "    data = pd.merge(data,dep, on=\"dep\")\n",
    "    \n",
    "    \n",
    "    # not yet eligeable for retirement\n",
    "    target_activity_types_y = [\"TACT1-1\", \"TACT1-2\", \"TACT2-2\", \"TACT2-4\", \"TACT2-5\"]\n",
    "\n",
    "    # Define the specific columns for retired individuals\n",
    "    retired_col = [\"Previous_occupation_42\", \"previous_emp_type\", \"previous_dep\", \"retirement_pay\"]\n",
    "\n",
    "    # Apply logic to fill NaN values in 'retirement_age' only if activity_type is in target_activity_types_y\n",
    "    data['retirement_age'] = data.apply(\n",
    "    lambda row: 'not_relevant' if pd.isna(row['retirement_age']) and row['activity_type'] in target_activity_types_y \n",
    "        else row['retirement_age'],\n",
    "        axis=1)\n",
    "\n",
    "    # Select columns ending with '_y'\n",
    "    columns_y = [col for col in data.columns if col.endswith('_y')]\n",
    "\n",
    "    # Combine the lists of columns to include the retired columns as well\n",
    "    all_target_columns = columns_y + retired_col\n",
    "    \n",
    "    # Loop through the combined list of columns (both '_y' and retired columns)\n",
    "    for col in all_target_columns:\n",
    "        # Check if the column exists in data\n",
    "        if col in data.columns:\n",
    "            # Check if the column is numerical\n",
    "            if pd.api.types.is_numeric_dtype(data[col]):\n",
    "                # For numerical columns, set 0 for matching activity types, keep existing values otherwise\n",
    "                data[col] = data.apply(\n",
    "                    lambda row: 0 if row[\"activity_type\"] in target_activity_types_y and pd.isna(row[col]) else row[col],\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                # For non-numerical columns, set 'not_relevant' for matching activity types\n",
    "                data[col] = data.apply(\n",
    "                    lambda row: \"not_relevant\" if row[\"activity_type\"] in target_activity_types_y and pd.isna(row[col]) else row[col],\n",
    "                    axis=1\n",
    "                )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## There is a special group of 251 observations that are considered retired but that with most likelihood never worked.\n",
    "    ## which very likely is people that has never worked due to some reason (old housewifes, institutionalized for instance)\n",
    "    # this would explain lack of data regarding previous work\n",
    "    \n",
    "    \n",
    "    csp_codes = [\"csp_8_5\", \"csp_8_6\"]\n",
    "    \n",
    "    # Function to apply the changes\n",
    "    def fill_missing_values(row):\n",
    "        # Check if Occupation_42 is one of the csp codes and the columns are NaN\n",
    "        if row[\"Occupation_42\"] in csp_codes:\n",
    "            if pd.isna(row[\"Previous_occupation_42\"]):\n",
    "                row[\"Previous_occupation_42\"] = \"never worked\"\n",
    "            if pd.isna(row[\"previous_emp_type\"]):\n",
    "                row[\"previous_emp_type\"] = \"never worked\"\n",
    "            if pd.isna(row[\"retirement_age\"]):\n",
    "                row[\"retirement_age\"] = \"never worked\"\n",
    "        return row\n",
    "    \n",
    "    # Apply the function to the DataFrame\n",
    "    data = data.apply(fill_missing_values, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##\n",
    "    ## Not working and will therefore not have any values for working related questions. \n",
    "    target_activity_types = [\"TACT2-1\", \"TACT1-2\",\"TACT2-2\", \"TACT2-4\", \"TACT2-5\"]\n",
    "    \n",
    "    data.loc[data[\"activity_type\"].isin(target_activity_types) & data[\"PAY\"].isna(), \"PAY\"] = 0\n",
    "    data.loc[data[\"activity_type\"].isin(target_activity_types) & data[\"emp_type\"].isna(), \"emp_type\"] = \"not_relevant\"\n",
    "    \n",
    "    # Select columns ending with '_x'\n",
    "    columns_x = [col for col in data.columns if col.endswith('_x')]\n",
    "    \n",
    "    # Loop through the selected columns\n",
    "    for col in columns_x:\n",
    "        # Check if the column is numerical\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            # For numerical columns, set 0 for matching activity types, keep existing values otherwise\n",
    "            data[col] = data.apply(\n",
    "                lambda row: 0 if row[\"activity_type\"] in target_activity_types and pd.isna(row[col]) else row[col],\n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            # For non-numerical columns, set 'not_relevant' for matching activity types\n",
    "            data[col] = data.apply(\n",
    "                lambda row: \"not_relevant\" if row[\"activity_type\"] in target_activity_types and pd.isna(row[col]) else row[col],\n",
    "                axis=1\n",
    "            )\n",
    "   \n",
    "    \n",
    "    ## Sport people are either \n",
    "    data.loc[:, \"SPORTS\"] = data.loc[:, \"SPORTS\"].fillna(\"not_registered\")\n",
    "\n",
    "\n",
    "    ###\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2-1', na=False), 'employer_type_x'] = \"ct_6\"\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2-2', na=False), 'employer_type_x'] = \"ct_9\"\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2-3', na=False), 'employer_type_x'] = \"ct_9\"\n",
    "    \n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2-1', na=False), 'employer_type_y'] = \"ct_6\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2-2', na=False), 'employer_type_y'] = \"ct_9\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2-3', na=False), 'employer_type_y'] = \"ct_9\"\n",
    "\n",
    "\n",
    "    ###\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2', na=False), 'work_condition_x'] = \"N\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2', na=False), 'work_condition_y'] = \"N\"\n",
    "    \n",
    "    ###\n",
    "    data.loc[data['work_desc_x'].str.startswith('231a', na=False), 'employee_count_x'] = \"tr_6\"\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2-1', na=False), 'employee_count_x'] = \"tr_1\"\n",
    "    \n",
    "    data.loc[data['work_desc_y'].str.startswith('231a', na=False), 'employee_count_y'] = \"tr_6\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2-1', na=False), 'employee_count_y'] = \"tr_1\"\n",
    "    \n",
    "    ###\n",
    "    data.loc[data['emp_type'].str.startswith('ec-1-6', na=False), 'Type_of_contract_x'] = \"CDI\"\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2', na=False), 'Type_of_contract_x'] = \"No contract\"\n",
    "    \n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-1-6', na=False), 'Type_of_contract_y'] = \"CDI\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2', na=False), 'Type_of_contract_y'] = \"No contract\"\n",
    "\n",
    "\n",
    "    ###\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2', na=False), 'JOB_CATEGORY_x'] = \"not employee\"\n",
    "    data.loc[data['emp_type'].str.startswith('ec-1-6', na=False), 'JOB_CATEGORY_x'] = \"O\"\n",
    "    \n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2', na=False), 'JOB_CATEGORY_y'] = \"not employee\"\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-1-6', na=False), 'JOB_CATEGORY_y'] = \"O\"\n",
    "\n",
    "    ###\n",
    "    data.loc[data['emp_type'].str.startswith('ec-2', na=False), 'PAY'] = 0\n",
    "\n",
    "    data.loc[data['previous_emp_type'].str.startswith('ec-2', na=False), 'retirement_pay'] = 0\n",
    "\n",
    "\n",
    "    ###\n",
    "    data['working_but_nocon_income'] = data['emp_type'].apply(\n",
    "        lambda x: 1 if str(x).startswith('ec-2') else 0)\n",
    "        \n",
    "    \n",
    "    #priests\n",
    "    data.loc[\n",
    "        (data['Occupation_42'] == 'csp_4_4') & data['PAY'].isna(), \n",
    "        ['PAY', 'working_but_nocon_income']\n",
    "    ] = [0, 1]\n",
    "    \n",
    "    data.loc[\n",
    "        (data['Occupation_42'] == 'csp_4_4') & data['employer_type_x'].isna(), \"employer_type_x\"] = \"ct_2\" \n",
    "    \n",
    "    #priests retired\n",
    "    data.loc[\n",
    "        (data['Previous_occupation_42'] == 'csp_4_4') & data['PAY'].isna(), \n",
    "        ['PAY', 'working_but_nocon_income']\n",
    "    ] = [0, 1]\n",
    "    \n",
    "    data.loc[\n",
    "        (data['Occupation_42'] == 'csp_4_4') & data['employer_type_x'].isna(), \"employer_type_x\"] = \"ct_2\" \n",
    "    \n",
    "    \n",
    "    data = data.drop([\"JOB_CATEGORY_x\", \"JOB_CATEGORY_y\"], axis=1)\n",
    "    \n",
    "    # Creating a new column 'fixed_contract' based on conditions applied to 'emp_type'\n",
    "    def assign_fixed_contract(emp_type):\n",
    "        if emp_type.startswith('ec-1') and emp_type != 'ec-1-6':\n",
    "            return 'yes'\n",
    "        elif emp_type == 'ec-1-6':\n",
    "            return 'no contract'\n",
    "        else:\n",
    "            return 'no'\n",
    "    \n",
    "    data['fixed_contract'] = data['emp_type'].apply(assign_fixed_contract)\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b33ba50f-a655-4ec3-b0d6-0066e2c22d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_training = preprocessing(merged_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3a221ba-9faf-4a80-8ca9-34117a937c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_training_sampled = merged_training.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "588a3c45-0f49-4f55-93a2-29a33a6e90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original dataset\n",
    "df = merged_training_sampled.copy()\n",
    "label_mapping = {'I': 0, 'N':1}\n",
    "df['target'] = df['target'].map(label_mapping)\n",
    "# Separate the target, features, and pkey\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Handle categorical features using Label Encoding\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    X[col] = encoder.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Split the dataset into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # Stratify based on the target variable\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6710ba6-aa4c-4547-b904-fde15f8ebf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Accuracy: 88.39%\n",
      "Precision: 78.56%\n",
      "Recall: 87.64%\n",
      "F1 Score: 82.85%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91      6806\n",
      "           1       0.79      0.88      0.83      3203\n",
      "\n",
      "    accuracy                           0.88     10009\n",
      "   macro avg       0.86      0.88      0.87     10009\n",
      "weighted avg       0.89      0.88      0.89     10009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy the original dataset\n",
    "# Instantiate the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Hyperparameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 30),  # Keep this as is, a larger learning rate often helps explore the parameter space better\n",
    "    'max_depth': [3, 5, 6, 7, 10, 12],  # Increase the depth range slightly to allow for more complex models\n",
    "    'n_estimators': [200, 500, 1000, 1500, 2000],  # Increase n_estimators to allow for more boosting rounds\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],  # No changes, subsample is important for preventing overfitting\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],  # No changes, colsample_bytree is also important for preventing overfitting\n",
    "    'gamma': [0, 0.1, 0.5, 1, 2, 5],  # Increased the range for gamma, which can help regularize and reduce overfitting\n",
    "    'min_child_weight': [1, 3, 5, 7, 10],  # Expanded the range to allow more fine-tuned choices\n",
    "    'scale_pos_weight': [1, 2, 5],  # This is useful if you have class imbalance\n",
    "    'max_delta_step': [0, 1, 5]  # This is useful to stabilize training when there's class imbalance or other issues\n",
    "}\n",
    "\n",
    "# Randomized search over the parameter grid\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model, param_distributions=param_dist,\n",
    "    n_iter=100, cv=3, verbose=1, n_jobs=-1, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from RandomizedSearchCV\n",
    "best_random_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_random_model.predict(X_test)  # Predicted labels\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4042ac7b-2e71-4821-92ae-e7993433b5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_xgb_model.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_random_model, 'best_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f406c1-e814-4fdf-92c4-8773eddc7c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9427882-113f-445e-9c90-1dca232acb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Accuracy: 79.32%\n",
      "Precision: 75.36%\n",
      "Recall: 50.64%\n",
      "F1 Score: 60.57%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86       687\n",
      "           1       0.75      0.51      0.61       314\n",
      "\n",
      "    accuracy                           0.79      1001\n",
      "   macro avg       0.78      0.72      0.73      1001\n",
      "weighted avg       0.79      0.79      0.78      1001\n",
      "\n",
      "Best Hyperparameters from RandomizedSearchCV:\n",
      "{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 40, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500, 1000],  # Number of trees\n",
    "    'max_depth': [1, 10, 20, 30, 40],  # Depth of the tree\n",
    "    'min_samples_split': [2, 5, 10, 20],  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4, 10],  # Minimum samples required to be at a leaf node\n",
    "    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of random combinations to try\n",
    "    cv=3,  # Number of cross-validation folds\n",
    "    verbose=2,  # Display the progress\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='roc_auc'  # Evaluate using ROC AUC\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the random search\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf)\n",
    "recall = recall_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters from RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1f54e4c-6eaf-4ef3-96c9-63853268a498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_rf_model.pkl']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_rf_model, 'best_rf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dee59b1-fe52-4993-81b9-8b3d8fe83734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_predict(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Handle categorical features using Label Encoding\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "    # Predict numeric target using the trained model\n",
    "    df[\"target\"] = best_random_model.predict(df)\n",
    "    \n",
    "    # Define label mapping\n",
    "    label_mapping = {0: 'I', 1: 'N'}\n",
    "    \n",
    "    # Map numeric predictions to categorical labels\n",
    "    df[\"target\"] = df[\"target\"].map(label_mapping)\n",
    "\n",
    "    df = df[[\"pkey\",\"target\"]]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efc11403-a5b1-4bbb-ab8b-14b46d2f39ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['pkey', 'Occupation_42', 'Is_student', 'activity_type', 'INSEE', 'AGE_2018', 'DEGREE', 'SEX', 'household_type', 'emp_type', 'Type_of_contract_x', 'PAY', 'job_dep_x', 'employer_type_x', 'employee_count_x', 'work_condition_x', 'ECO_SECT_x', 'working_hours_x', 'work_desc_x', 'Previous_occupation_42', 'previous_emp_type', 'retirement_age', 'ECO_SECT_y', 'previous_dep', 'job_dep_y', 'work_desc_y', 'Type_of_contract_y', 'work_condition_y', 'employer_type_y', 'working_hours_y', 'employee_count_y', 'retirement_pay', 'SPORTS', 'Nom de la commune', 'Town_type', 'dep', 'X', 'Y', 'LAT', 'LONG', 'Inhabitants', 'Nom du département', 'REG', 'working_but_nocon_income', 'fixed_contract'] ['pkey', 'Occupation_42', 'Is_student', 'activity_type', 'INSEE', 'AGE_2018', 'DEGREE', 'SEX', 'household_type', 'emp_type', 'Type_of_contract_x', 'PAY', 'job_dep_x', 'employer_type_x', 'employee_count_x', 'work_condition_x', 'ECO_SECT_x', 'JOB_CATEGORY_x', 'working_hours_x', 'work_desc_x', 'Previous_occupation_42', 'previous_emp_type', 'retirement_age', 'ECO_SECT_y', 'previous_dep', 'job_dep_y', 'work_desc_y', 'Type_of_contract_y', 'JOB_CATEGORY_y', 'work_condition_y', 'employer_type_y', 'working_hours_y', 'employee_count_y', 'retirement_pay', 'SPORTS']\nexpected fixed_contract, LONG, dep, Nom de la commune, Town_type, REG, Inhabitants, Nom du département, Y, X, working_but_nocon_income, LAT in input data\ntraining data did not have the following fields: JOB_CATEGORY_y, JOB_CATEGORY_x",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m xg_predict(merged_test)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m, in \u001b[0;36mxg_predict\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(df[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Predict numeric target using the trained model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m best_random_model\u001b[38;5;241m.\u001b[39mpredict(df)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define label mapping\u001b[39;00m\n\u001b[0;32m     13\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1565\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1562\u001b[0m     iteration_range: Optional[IterationRange] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1563\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[0;32m   1564\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m-> 1565\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   1566\u001b[0m             X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1567\u001b[0m             output_margin\u001b[38;5;241m=\u001b[39moutput_margin,\n\u001b[0;32m   1568\u001b[0m             validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   1569\u001b[0m             base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1570\u001b[0m             iteration_range\u001b[38;5;241m=\u001b[39miteration_range,\n\u001b[0;32m   1571\u001b[0m         )\n\u001b[0;32m   1572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[0;32m   1573\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39minplace_predict(\n\u001b[0;32m   1187\u001b[0m             data\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1188\u001b[0m             iteration_range\u001b[38;5;241m=\u001b[39miteration_range,\n\u001b[0;32m   1189\u001b[0m             predict_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_margin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1190\u001b[0m             missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1191\u001b[0m             base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1192\u001b[0m             validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   1193\u001b[0m         )\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2514\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2512\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[0;32m   2513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[1;32m-> 2514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(fns)\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[0;32m   2516\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:3079\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[0;32m   3074\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   3075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3076\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[0;32m   3077\u001b[0m     )\n\u001b[1;32m-> 3079\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['pkey', 'Occupation_42', 'Is_student', 'activity_type', 'INSEE', 'AGE_2018', 'DEGREE', 'SEX', 'household_type', 'emp_type', 'Type_of_contract_x', 'PAY', 'job_dep_x', 'employer_type_x', 'employee_count_x', 'work_condition_x', 'ECO_SECT_x', 'working_hours_x', 'work_desc_x', 'Previous_occupation_42', 'previous_emp_type', 'retirement_age', 'ECO_SECT_y', 'previous_dep', 'job_dep_y', 'work_desc_y', 'Type_of_contract_y', 'work_condition_y', 'employer_type_y', 'working_hours_y', 'employee_count_y', 'retirement_pay', 'SPORTS', 'Nom de la commune', 'Town_type', 'dep', 'X', 'Y', 'LAT', 'LONG', 'Inhabitants', 'Nom du département', 'REG', 'working_but_nocon_income', 'fixed_contract'] ['pkey', 'Occupation_42', 'Is_student', 'activity_type', 'INSEE', 'AGE_2018', 'DEGREE', 'SEX', 'household_type', 'emp_type', 'Type_of_contract_x', 'PAY', 'job_dep_x', 'employer_type_x', 'employee_count_x', 'work_condition_x', 'ECO_SECT_x', 'JOB_CATEGORY_x', 'working_hours_x', 'work_desc_x', 'Previous_occupation_42', 'previous_emp_type', 'retirement_age', 'ECO_SECT_y', 'previous_dep', 'job_dep_y', 'work_desc_y', 'Type_of_contract_y', 'JOB_CATEGORY_y', 'work_condition_y', 'employer_type_y', 'working_hours_y', 'employee_count_y', 'retirement_pay', 'SPORTS']\nexpected fixed_contract, LONG, dep, Nom de la commune, Town_type, REG, Inhabitants, Nom du département, Y, X, working_but_nocon_income, LAT in input data\ntraining data did not have the following fields: JOB_CATEGORY_y, JOB_CATEGORY_x"
     ]
    }
   ],
   "source": [
    "# save data\n",
    "xg_predict(merged_test).to_csv('predictions.csv', index=False, sep=',', decimal='.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
